{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Reference Notebook\n",
    "\n",
    "pyenv name: ner-chatbot-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze vs Unsqueeze\n",
    "\n",
    "Unsqueeze: adds one dimensiion to embedding\n",
    "\n",
    "Squeeze: removes one dimension to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4,5,6,7,8]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4, 5, 6, 7, 8]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.unsqueeze(x, 0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.squeeze(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickstart \n",
    "\n",
    "Link (https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n",
    "\n",
    "Link (https://pytorch.org/text/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch has two approaches to handling datasets:\n",
    "\n",
    "- DataLoader from torch.utils.data\n",
    "    - Wraps an iterable around the Dataset\n",
    "- Dataset from torch.utils.data\n",
    "    - Stores all samples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets \n",
    "from torchvision.transforms import ToTensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets offer a wide range of datasets pertaining to text, audio, and vision \n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "testing_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate a DataLoader Class by passing a Dataset Class to it.\n",
    "# This will wrap an iterable over the dataset to handle \n",
    "# ...batching of data, shuffling, multiprocess data loading, and sampling\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset FashionMNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " Dataset FashionMNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: ToTensor())"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset, test_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataloader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a model in Pytorch, it must be defined by a class that inherits from the torch.nn.Module class. Our model class should include an __init__ and forward method. The __init__ function will define each layer of the model and the forward function will connect the layers together of which breaks down the runthrough of the input and hidden states passed from layer to layer. Additionally, it helps to utlize a GPU or MPS rather than a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTClassifier(\n",
       "  (layer_1_flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (layer_2_linear_relu): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FashionMNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1_flatten = nn.Flatten()\n",
    "        self.layer_2_linear_relu = nn.Sequential(\n",
    "            nn.Linear(in_features=(28*28), out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            # RuntimeError: linear(): \n",
    "            # input and weight.T shapes cannot be \n",
    "            # multiplied (64x64 and 512x10)\n",
    "\n",
    "            # nn.Linear(in_features=512, out_features=64),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_states_1 = self.layer_1_flatten(x)\n",
    "        logits_2 = self.layer_2_linear_relu(hidden_states_1)\n",
    "        return logits_2\n",
    "    \n",
    "model = FashionMNISTClassifier().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CrossEntropyLoss(),\n",
       " SGD (\n",
       " Parameter Group 0\n",
       "     dampening: 0\n",
       "     differentiable: False\n",
       "     foreach: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     momentum: 0\n",
       "     nesterov: False\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish the loss function and optimizer to use\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-3)\n",
    "loss, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batches of data are fed into the model to make predictions on training set, then backpropagates the prediction error to adjust the parameters of the model. I.e. the model computes the loss and then adjusting the parameters through back propagation to minimize that said loss. Furthermore, be sure to set training/testing data to the same .to(device) as the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_train(dataloader, model, loss_fn, optimizer):\n",
    "    # size of dataset\n",
    "    len_train = train_dataloader.dataset.data.shape[0]\n",
    "    # Compute loss\n",
    "    temp_loss = 0\n",
    "    # train model\n",
    "    model.train()\n",
    "    # run a training loop that computes loss, backpropogates error, then adjusts the parameters\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Compute the model predictions over the training features\n",
    "        pred = model(x)\n",
    "        # Compute the loss based on the predicted labels and the true labels\n",
    "        loss = loss_fn(pred,y)\n",
    "        temp_loss += loss\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Take a step in some direction on the loss surface that helps lower the prediction loss\n",
    "        optimizer.step\n",
    "        # Zero out the gradients since the model is not influenced by past results. \n",
    "        # Note there is no need to keep the past gradients when that info was used\n",
    "        # ...to update the location of the optimizer on the loss surface. \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(f\"Batch: {batch}, Loss: {temp_loss}\", \"\\n\")\n",
    "\n",
    "        # if batch % 100 == 0:\n",
    "        #     loss, current = loss.item(), (batch+1)*len(x)\n",
    "        #     print(f\"Loss: {loss}, Samples {current/len_train}\")\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{len_train:>5d}]\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_test(dataloader, model, loss_fn):\n",
    "    len_test = test_dataloader.dataset.data.shape[0]\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred_y = model(x)\n",
    "            test_loss += loss_fn(pred_y, y).item()\n",
    "            correct += (pred_y.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= len_test\n",
    "    # print(f\"Testing: \\n Accuracy: {correct} \\n Loss: {test_loss}\")\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model means to learn the patterns of data and the parameters to make optimal predictions. Training occurs over several epochs, where each epoch trains on a batch of data. Notice in the code, the training function prints out the loss and the number of samples used. The test set prints out the accuracy and loss for each training batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FashionMNISTClassifier(\n",
       "   (layer_1_flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   (layer_2_linear_relu): Sequential(\n",
       "     (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "   )\n",
       " ),\n",
       " CrossEntropyLoss(),\n",
       " SGD (\n",
       " Parameter Group 0\n",
       "     dampening: 0\n",
       "     differentiable: False\n",
       "     foreach: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     momentum: 0\n",
       "     nesterov: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2a1460670>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, loss, optimizer, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "loss: 2.306123  [   64/60000]\n",
      "loss: 2.309023  [ 6464/60000]\n",
      "loss: 2.309962  [12864/60000]\n",
      "loss: 2.310167  [19264/60000]\n",
      "loss: 2.301754  [25664/60000]\n",
      "loss: 2.300050  [32064/60000]\n",
      "loss: 2.305031  [38464/60000]\n",
      "loss: 2.308173  [44864/60000]\n",
      "loss: 2.311109  [51264/60000]\n",
      "loss: 2.285941  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.3%, Avg loss: 2.303915 \n",
      "\n",
      "Epoch: 2\n",
      "loss: 2.306123  [   64/60000]\n",
      "loss: 2.309023  [ 6464/60000]\n",
      "loss: 2.309962  [12864/60000]\n",
      "loss: 2.310167  [19264/60000]\n",
      "loss: 2.301754  [25664/60000]\n",
      "loss: 2.300050  [32064/60000]\n",
      "loss: 2.305031  [38464/60000]\n",
      "loss: 2.308173  [44864/60000]\n",
      "loss: 2.311109  [51264/60000]\n",
      "loss: 2.285941  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.3%, Avg loss: 2.303915 \n",
      "\n",
      "Epoch: 3\n",
      "loss: 2.306123  [   64/60000]\n",
      "loss: 2.309023  [ 6464/60000]\n",
      "loss: 2.309962  [12864/60000]\n",
      "loss: 2.310167  [19264/60000]\n",
      "loss: 2.301754  [25664/60000]\n",
      "loss: 2.300050  [32064/60000]\n",
      "loss: 2.305031  [38464/60000]\n",
      "loss: 2.308173  [44864/60000]\n",
      "loss: 2.311109  [51264/60000]\n",
      "loss: 2.285941  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.3%, Avg loss: 2.303915 \n",
      "\n",
      "Epoch: 4\n",
      "loss: 2.306123  [   64/60000]\n",
      "loss: 2.309023  [ 6464/60000]\n",
      "loss: 2.309962  [12864/60000]\n",
      "loss: 2.310167  [19264/60000]\n",
      "loss: 2.301754  [25664/60000]\n",
      "loss: 2.300050  [32064/60000]\n",
      "loss: 2.305031  [38464/60000]\n",
      "loss: 2.308173  [44864/60000]\n",
      "loss: 2.311109  [51264/60000]\n",
      "loss: 2.285941  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.3%, Avg loss: 2.303915 \n",
      "\n",
      "Epoch: 5\n",
      "loss: 2.306123  [   64/60000]\n",
      "loss: 2.309023  [ 6464/60000]\n",
      "loss: 2.309962  [12864/60000]\n",
      "loss: 2.310167  [19264/60000]\n",
      "loss: 2.301754  [25664/60000]\n",
      "loss: 2.300050  [32064/60000]\n",
      "loss: 2.305031  [38464/60000]\n",
      "loss: 2.308173  [44864/60000]\n",
      "loss: 2.311109  [51264/60000]\n",
      "loss: 2.285941  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.3%, Avg loss: 2.303915 \n",
      "\n",
      "Fin\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    ner_train(train_dataloader, model, loss, optimizer)\n",
    "    ner_test(test_dataloader, model, loss)\n",
    "print('Fin')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model and its parameters in an internal state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/druestaples/.pyenv/versions/3.8.12/envs/ner-chatbot-env/lib/python3.8/site-packages/torch/_tensor_str.py:137: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:283.)\n",
      "  nonzero_finite_vals = torch.masked_select(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer_2_linear_relu.0.weight',\n",
       "              tensor([[-0.0196, -0.0325, -0.0176,  ...,  0.0033, -0.0076, -0.0189],\n",
       "                      [-0.0199,  0.0266,  0.0286,  ..., -0.0157,  0.0154, -0.0274],\n",
       "                      [ 0.0039, -0.0239,  0.0177,  ..., -0.0039, -0.0232, -0.0278],\n",
       "                      ...,\n",
       "                      [-0.0288,  0.0055,  0.0166,  ...,  0.0208, -0.0169,  0.0231],\n",
       "                      [-0.0301,  0.0011,  0.0143,  ...,  0.0187, -0.0289,  0.0303],\n",
       "                      [ 0.0227, -0.0251,  0.0306,  ...,  0.0211, -0.0037,  0.0220]],\n",
       "                     device='mps:0')),\n",
       "             ('layer_2_linear_relu.0.bias',\n",
       "              tensor([-1.0901e-03, -1.8960e-02,  2.2796e-03, -3.5242e-02, -2.6809e-02,\n",
       "                      -3.4505e-02,  1.2617e-02, -2.5151e-02, -2.8374e-02,  9.3289e-03,\n",
       "                       4.0030e-03, -8.9758e-03, -2.2560e-02,  6.6295e-03,  1.3166e-02,\n",
       "                       2.5571e-02, -3.4357e-02,  3.0701e-03, -3.3995e-02,  6.8501e-03,\n",
       "                      -2.4734e-02,  2.9295e-02, -2.8250e-02, -2.5324e-02,  1.3596e-02,\n",
       "                      -1.5518e-02,  2.1618e-02,  2.9977e-02,  1.2388e-02,  2.7531e-02,\n",
       "                      -2.8061e-02, -9.3931e-03, -2.4845e-02,  6.7194e-03, -2.2090e-02,\n",
       "                      -7.9227e-03,  3.0610e-02, -7.3237e-03,  1.2524e-02, -5.7642e-04,\n",
       "                       1.3755e-02,  2.5869e-02,  2.5077e-02, -1.0172e-02, -8.1191e-03,\n",
       "                       3.1262e-03, -6.4745e-03, -3.9874e-03,  1.5380e-02,  2.2705e-03,\n",
       "                       1.1217e-02, -2.6439e-03,  3.1493e-02,  1.6171e-02,  1.0446e-02,\n",
       "                       1.8671e-02, -5.8877e-04,  3.2638e-02,  1.0766e-02, -4.0050e-03,\n",
       "                       3.0394e-02, -4.7407e-03, -3.2948e-02,  2.8388e-02, -3.3176e-02,\n",
       "                      -2.5104e-02, -1.0670e-02, -7.3725e-03, -4.1147e-03,  1.9666e-02,\n",
       "                      -2.5329e-02, -3.1522e-02, -3.4643e-02,  3.3084e-02,  2.0364e-02,\n",
       "                      -2.7655e-02,  1.2257e-02, -1.6018e-02, -1.3534e-02, -6.7166e-04,\n",
       "                       1.8023e-02, -2.6672e-02,  1.6086e-02, -9.4904e-03,  1.4406e-02,\n",
       "                       1.4978e-02, -5.0295e-04, -2.8292e-02,  2.9250e-02,  1.0447e-02,\n",
       "                       2.0632e-02,  9.0473e-03, -1.7442e-02,  2.9699e-02, -2.0449e-02,\n",
       "                       1.2656e-02,  1.5882e-02,  5.4524e-03,  1.9161e-02,  2.8974e-03,\n",
       "                      -1.9404e-02, -2.6591e-02,  2.7747e-02, -1.9806e-02,  2.1711e-02,\n",
       "                       9.7048e-03, -2.8808e-02, -6.3156e-03,  1.9671e-03,  2.4623e-02,\n",
       "                      -1.8472e-02, -3.2143e-02,  1.9536e-02,  1.7552e-03, -3.1047e-02,\n",
       "                      -9.6003e-03,  8.8613e-03,  1.9456e-02,  4.9778e-03, -3.1553e-02,\n",
       "                       3.4384e-02,  2.9371e-02, -1.1461e-02,  3.2545e-02, -3.0121e-04,\n",
       "                       7.7661e-03,  8.9360e-04,  1.2941e-02,  1.8031e-03,  1.3977e-02,\n",
       "                       9.1375e-03,  1.8470e-02,  3.2157e-02, -3.2278e-02, -7.6559e-03,\n",
       "                       2.7768e-02,  9.1902e-04,  7.4387e-03,  2.5094e-02,  3.0977e-02,\n",
       "                       2.8418e-03, -2.4377e-02, -1.4952e-03, -2.0458e-02,  2.8471e-02,\n",
       "                       2.9268e-02,  1.5941e-03, -3.1318e-02, -2.2654e-02,  1.4273e-02,\n",
       "                       1.1841e-02, -9.7638e-03, -2.2215e-02,  3.6207e-03,  2.2699e-02,\n",
       "                       3.1381e-02, -2.7261e-02, -1.5323e-02, -2.7552e-02, -2.8689e-02,\n",
       "                      -8.9878e-03, -2.2639e-02, -1.1976e-02, -2.2441e-02, -5.6898e-03,\n",
       "                      -2.5307e-02,  2.3176e-02, -2.2265e-02,  8.1875e-05,  3.4293e-02,\n",
       "                      -3.4238e-02, -4.2312e-03,  2.1391e-02, -8.9886e-03, -2.5040e-02,\n",
       "                       1.7122e-04, -1.8252e-02, -2.9841e-02, -1.0843e-02, -2.8078e-02,\n",
       "                       1.7194e-02,  1.7798e-02, -3.3953e-02, -2.4212e-02,  2.9289e-02,\n",
       "                      -1.6001e-02,  2.1021e-02,  1.3087e-02,  1.0123e-02, -3.5212e-02,\n",
       "                      -1.8395e-02, -3.5467e-02,  1.3346e-02, -2.0240e-02, -2.3813e-02,\n",
       "                      -3.0279e-02,  1.7285e-02,  2.7690e-02, -1.2232e-02, -2.1391e-02,\n",
       "                      -3.4626e-02, -3.2715e-02, -2.6606e-02,  8.8320e-04,  2.3806e-02,\n",
       "                      -1.1058e-02,  1.6055e-02, -3.4847e-02, -2.1708e-02,  1.3124e-02,\n",
       "                       2.3248e-02, -2.3835e-02,  3.5218e-02, -2.4566e-02, -2.8011e-02,\n",
       "                      -2.2666e-02, -1.4231e-02,  2.2253e-03, -1.6598e-03, -2.1951e-02,\n",
       "                       3.5453e-02, -1.2007e-02, -1.3221e-02,  3.4637e-02, -2.6709e-03,\n",
       "                       3.5260e-04, -2.0713e-02,  1.5370e-02,  2.7011e-02, -2.8574e-02,\n",
       "                       2.5515e-02,  9.2994e-03, -7.7476e-03,  3.4655e-02,  1.4121e-02,\n",
       "                       1.2957e-02, -2.2451e-02,  5.9467e-03,  3.2980e-02,  3.1218e-02,\n",
       "                      -1.4668e-02, -1.3868e-02,  2.8336e-02,  2.2735e-02, -9.6815e-05,\n",
       "                       3.5194e-02,  3.0546e-02,  2.2499e-02,  1.4715e-02, -1.5726e-02,\n",
       "                      -1.8372e-02, -2.9353e-02,  6.8805e-03, -3.1316e-02, -2.2652e-02,\n",
       "                      -1.7285e-02, -3.6736e-04, -3.2395e-02,  3.2321e-04,  2.6795e-02,\n",
       "                       3.0497e-03,  2.8802e-02, -7.4252e-03, -2.6665e-02,  1.3162e-03,\n",
       "                       1.2437e-02,  3.5474e-02,  7.6043e-03,  2.5109e-02, -2.1308e-02,\n",
       "                       2.4823e-02,  3.1607e-02, -3.8212e-03,  3.3896e-02,  9.9015e-03,\n",
       "                       1.7830e-03, -8.6993e-03, -1.9536e-02,  2.3989e-02,  2.1979e-02,\n",
       "                      -2.9419e-02, -1.7615e-02,  9.9880e-03, -2.6361e-02, -7.9489e-03,\n",
       "                      -2.3925e-02,  4.6386e-03,  7.6123e-03,  1.6124e-02,  3.2766e-02,\n",
       "                      -2.2631e-02,  3.0878e-02, -3.1978e-02,  2.6833e-02, -1.9022e-03,\n",
       "                      -1.4588e-02,  7.0382e-03,  2.5538e-02, -2.5402e-02,  3.2378e-02,\n",
       "                       9.9914e-03, -2.2476e-02, -2.0986e-02,  2.2003e-02, -1.8781e-02,\n",
       "                      -1.9077e-02, -7.9532e-03, -2.6654e-02, -2.0421e-02,  3.2056e-02,\n",
       "                      -8.1412e-03,  3.3686e-02,  3.0704e-02, -2.2692e-02, -2.8540e-02,\n",
       "                       6.0101e-03,  3.1636e-02, -3.5428e-03,  2.0407e-02,  3.2657e-04,\n",
       "                      -2.0876e-02, -5.6208e-04,  7.7783e-04,  2.3480e-03,  5.8684e-03,\n",
       "                      -3.4857e-02, -2.5656e-02,  3.4109e-02, -1.5583e-02, -3.4034e-03,\n",
       "                       9.4316e-03,  2.9554e-04,  1.6091e-02, -2.2185e-02, -1.6771e-02,\n",
       "                       3.3336e-02, -6.4108e-03,  2.4843e-03, -2.8753e-02,  1.2221e-03,\n",
       "                       2.4713e-02, -3.5032e-02, -2.2246e-02, -1.1611e-02, -1.2584e-02,\n",
       "                       3.3081e-02,  1.9034e-02,  1.6160e-02, -1.6379e-02, -1.8612e-02,\n",
       "                       5.2710e-03, -6.5820e-03, -1.4558e-03, -7.1019e-03,  8.1498e-03,\n",
       "                      -7.2034e-03,  3.4087e-02, -3.1989e-03, -2.4329e-02,  2.2161e-03,\n",
       "                       2.6046e-02, -1.9247e-02, -2.7217e-02, -1.5378e-02,  3.5116e-02,\n",
       "                       3.0659e-02, -1.5945e-02, -2.7767e-02, -2.9902e-02, -9.3753e-03,\n",
       "                      -3.0983e-02,  1.7991e-02, -3.1120e-02, -1.9119e-03, -2.6875e-02,\n",
       "                       3.1565e-02,  2.1690e-02, -2.1414e-02, -4.0261e-03, -2.5207e-02,\n",
       "                      -2.7515e-02, -2.5535e-02,  1.4963e-03, -2.8347e-02,  8.0297e-03,\n",
       "                      -3.1923e-02,  1.8954e-02,  2.7045e-02, -3.7654e-03,  3.5651e-02,\n",
       "                      -2.8180e-02,  2.5028e-02, -3.5142e-02,  3.2652e-02, -2.4979e-02,\n",
       "                       1.2565e-02, -2.3977e-02,  3.5361e-02, -3.5624e-02,  2.0741e-02,\n",
       "                       2.2271e-02,  1.7016e-03,  1.1428e-02,  3.2600e-02,  8.7430e-03,\n",
       "                      -1.2133e-02, -9.0790e-03,  1.7135e-02,  3.1113e-02,  3.3155e-02,\n",
       "                      -8.9609e-03,  2.8368e-03, -2.7913e-02,  2.0825e-02, -2.6626e-02,\n",
       "                      -8.9011e-03,  2.3779e-02,  4.1941e-03,  1.4640e-02, -1.8226e-02,\n",
       "                       1.2258e-02, -3.2276e-02,  1.6537e-02, -3.3669e-02, -1.2170e-02,\n",
       "                       2.3794e-03, -9.5157e-03,  2.3546e-03, -1.8264e-03, -2.4883e-02,\n",
       "                      -2.8967e-02,  1.2568e-02, -2.3003e-02,  3.0818e-02, -1.1239e-02,\n",
       "                      -2.8912e-02, -2.9738e-02, -2.2613e-02, -2.4901e-02,  3.5374e-02,\n",
       "                      -1.9377e-02, -1.1321e-03,  1.5617e-02, -2.6265e-04, -1.3224e-03,\n",
       "                      -3.1300e-02, -1.8526e-02,  3.0636e-02, -3.0112e-02, -1.2519e-02,\n",
       "                       1.5117e-02,  7.8027e-05,  1.7715e-03, -1.7382e-02, -1.4614e-02,\n",
       "                       1.1728e-02,  2.8016e-02,  3.1475e-03,  2.5293e-02,  2.8756e-03,\n",
       "                       1.2417e-02, -2.6222e-02,  1.5018e-02,  1.9808e-02,  2.8956e-02,\n",
       "                      -4.6282e-03,  2.5626e-02, -2.8166e-02,  3.0308e-02, -1.9897e-02,\n",
       "                      -2.1675e-02, -3.1311e-02,  2.7902e-02, -4.5586e-03,  2.5735e-02,\n",
       "                      -2.5052e-02,  1.7728e-02,  8.3143e-03, -1.6599e-02, -7.4080e-06,\n",
       "                      -3.5470e-02,  2.4222e-02, -3.2608e-02,  1.6389e-02, -2.1835e-02,\n",
       "                       1.3482e-02, -2.9492e-05, -1.0621e-03, -3.1462e-02, -3.3094e-02,\n",
       "                      -1.1278e-02, -2.5822e-02,  1.5764e-03, -3.6097e-03,  2.4746e-02,\n",
       "                       4.2218e-03, -2.4438e-03, -1.9698e-02,  1.4229e-02,  6.2576e-03,\n",
       "                      -3.0139e-02,  2.7028e-02, -1.1880e-02,  1.0706e-02,  2.2129e-02,\n",
       "                      -1.6206e-03,  2.1355e-02,  2.8734e-02, -1.2987e-02, -1.0207e-02,\n",
       "                      -3.4483e-02,  2.3454e-02], device='mps:0')),\n",
       "             ('layer_2_linear_relu.2.weight',\n",
       "              tensor([[ 0.0005, -0.0084, -0.0170,  ...,  0.0181,  0.0321,  0.0113],\n",
       "                      [-0.0301,  0.0134,  0.0102,  ..., -0.0440,  0.0413,  0.0356],\n",
       "                      [ 0.0037,  0.0212, -0.0273,  ...,  0.0296, -0.0043,  0.0420],\n",
       "                      ...,\n",
       "                      [ 0.0172,  0.0309, -0.0236,  ..., -0.0262, -0.0073, -0.0026],\n",
       "                      [ 0.0348, -0.0417,  0.0008,  ..., -0.0301, -0.0408,  0.0402],\n",
       "                      [ 0.0050, -0.0020, -0.0317,  ..., -0.0168, -0.0327, -0.0115]],\n",
       "                     device='mps:0')),\n",
       "             ('layer_2_linear_relu.2.bias',\n",
       "              tensor([-1.1115e-03, -1.2010e-02,  4.9804e-03, -8.8016e-03,  9.3628e-03,\n",
       "                      -2.3655e-02, -1.4607e-02,  3.0698e-02, -4.4151e-02,  2.9922e-02,\n",
       "                      -3.8973e-02, -9.2834e-03,  3.2100e-02,  2.0648e-02, -7.0847e-03,\n",
       "                       7.2200e-03, -3.1298e-02, -1.6959e-03,  4.1405e-02, -3.9378e-02,\n",
       "                       4.6712e-03, -3.5011e-02,  2.1871e-02,  1.7293e-02, -2.1652e-02,\n",
       "                      -1.9267e-02,  1.2788e-02, -1.1267e-02, -2.0244e-02, -2.1528e-02,\n",
       "                       8.5209e-03, -3.9333e-02,  2.7306e-02, -1.7327e-03, -3.5298e-02,\n",
       "                      -7.8981e-03, -3.4778e-04, -3.0982e-02, -4.9268e-03, -2.0648e-02,\n",
       "                      -2.8958e-02, -3.9326e-02,  3.7534e-02,  3.9841e-03,  3.1100e-02,\n",
       "                       3.7266e-02, -6.9038e-03,  3.8164e-02, -2.0387e-02,  1.0615e-02,\n",
       "                       4.2734e-02, -4.0860e-02,  3.6928e-03,  2.9176e-03,  2.9809e-02,\n",
       "                       1.8427e-02,  2.1067e-04,  1.0063e-02,  2.8545e-02, -3.7822e-02,\n",
       "                      -1.7781e-02, -2.3990e-02,  2.8227e-02,  2.8162e-02,  1.7770e-02,\n",
       "                      -2.8487e-02,  4.3577e-03,  2.6499e-02,  1.2293e-02, -2.1272e-02,\n",
       "                       2.4318e-02, -2.4573e-02, -4.0082e-02,  1.6211e-02,  2.6258e-02,\n",
       "                      -3.9462e-02, -3.5879e-02,  3.2361e-02,  6.1915e-03,  1.3293e-02,\n",
       "                      -2.6725e-02,  1.4515e-02, -4.3049e-02, -2.8806e-02,  3.1867e-02,\n",
       "                      -6.6880e-03, -3.8842e-02, -4.1535e-02,  3.5165e-02, -9.2403e-03,\n",
       "                       2.4201e-02,  2.0638e-02,  1.3433e-02,  9.4505e-04, -4.3593e-02,\n",
       "                      -2.2800e-03, -6.7168e-03,  8.8134e-03, -3.3160e-02,  3.4256e-02,\n",
       "                      -2.2563e-02, -2.3045e-04, -1.2219e-02, -1.2015e-02, -3.9236e-02,\n",
       "                       1.8221e-02,  8.3199e-03,  3.0317e-02, -3.4135e-02,  4.1484e-02,\n",
       "                      -1.1375e-02, -1.0149e-02, -3.0195e-02,  2.5913e-02, -1.2438e-02,\n",
       "                      -5.7009e-04,  2.6428e-02,  1.3463e-02, -3.9220e-02, -2.1180e-02,\n",
       "                       3.6489e-02,  2.8964e-02, -1.2898e-02,  1.4186e-02, -4.2122e-02,\n",
       "                      -4.2771e-02, -4.0771e-03,  4.3954e-02,  1.1799e-03,  3.7163e-02,\n",
       "                       3.3356e-02,  1.5124e-02, -3.9836e-02, -1.7679e-02,  2.8243e-02,\n",
       "                       1.9561e-02,  3.1289e-02, -7.9328e-03, -3.4090e-02, -2.4053e-02,\n",
       "                      -3.6332e-02, -3.6670e-02, -4.0682e-02,  7.1715e-03,  3.8407e-02,\n",
       "                      -4.1193e-02, -3.7225e-03,  4.2158e-02, -4.2903e-02,  3.1535e-02,\n",
       "                       3.1742e-02, -2.1755e-02, -2.8130e-02,  1.9358e-02, -2.4644e-02,\n",
       "                      -2.5783e-02,  2.9851e-03, -2.5728e-02, -4.4113e-02,  3.5129e-02,\n",
       "                       3.5259e-02, -4.0068e-02,  3.2703e-02, -4.9910e-03,  2.3970e-02,\n",
       "                      -2.5452e-02,  3.7378e-02, -2.3852e-02, -2.0875e-02,  1.5000e-02,\n",
       "                       2.6961e-02,  2.9863e-03, -3.6936e-02,  3.4368e-02, -9.2018e-03,\n",
       "                       2.1513e-02, -2.1657e-02,  2.5288e-02,  3.5998e-03, -2.6993e-02,\n",
       "                      -1.7183e-02,  1.7589e-02,  7.9364e-03, -3.9025e-02,  2.3600e-02,\n",
       "                      -1.0249e-02, -3.2770e-04,  3.6153e-02, -3.2225e-02,  1.1640e-02,\n",
       "                      -2.9354e-02, -8.3338e-03, -1.6551e-02, -2.4418e-02, -3.6291e-03,\n",
       "                       1.5408e-02,  2.1779e-02, -4.1737e-03, -2.6663e-02, -3.0669e-02,\n",
       "                       8.0647e-04,  2.5626e-03, -1.2955e-02,  2.1990e-02, -2.6042e-02,\n",
       "                      -9.4501e-03, -8.7039e-03, -3.0302e-02, -3.7566e-02, -3.5941e-02,\n",
       "                       3.4647e-02,  2.4037e-02,  2.9907e-02, -1.3525e-02,  3.1953e-02,\n",
       "                      -5.4744e-03,  1.5940e-02,  4.0208e-02,  2.4788e-02,  3.4548e-02,\n",
       "                      -4.2626e-03, -2.4973e-03,  3.8159e-02,  1.3203e-02,  2.1008e-02,\n",
       "                      -1.4526e-02,  4.3584e-04, -2.9794e-02, -2.4013e-02, -3.5707e-02,\n",
       "                      -1.8803e-02, -7.9576e-04, -8.5010e-03, -3.9505e-02, -4.0793e-02,\n",
       "                       3.4546e-02,  2.4499e-02,  2.8462e-03, -4.1714e-02, -1.5599e-02,\n",
       "                      -3.1241e-02,  3.3956e-02, -4.2908e-02,  9.2726e-03, -4.2247e-02,\n",
       "                      -2.3882e-02,  3.4541e-02, -2.5699e-02,  2.8558e-02, -1.8388e-02,\n",
       "                       3.8345e-02, -1.3745e-02,  2.4766e-02,  4.9573e-03,  1.5562e-02,\n",
       "                       2.2564e-02, -2.3196e-02, -7.1964e-03, -8.8125e-03,  2.0474e-02,\n",
       "                       3.6852e-02, -1.4956e-02,  1.7427e-02,  3.8818e-02, -6.3820e-03,\n",
       "                      -2.6456e-02, -2.0423e-02,  2.6720e-02,  2.4637e-02,  3.3819e-02,\n",
       "                      -2.8077e-02, -4.3023e-02, -7.2781e-03, -2.6351e-02, -3.0657e-02,\n",
       "                       2.1419e-03, -3.0822e-02, -3.3871e-02,  1.7358e-02,  4.2551e-02,\n",
       "                       1.6646e-02,  3.7943e-02,  3.7216e-02, -1.4308e-02, -1.1499e-02,\n",
       "                      -3.6274e-02,  3.4464e-02,  1.1809e-02, -1.6351e-02, -4.3531e-02,\n",
       "                      -1.1267e-02, -3.0422e-02, -2.6915e-02,  3.1561e-02,  1.3604e-02,\n",
       "                       4.2190e-02,  3.6668e-02,  4.2213e-02, -3.3293e-02, -2.2899e-02,\n",
       "                      -2.8075e-02,  1.2260e-02, -4.2627e-03, -1.3382e-02, -1.6127e-02,\n",
       "                      -2.5813e-02, -4.1281e-02,  6.0275e-03, -1.8699e-02,  9.3982e-03,\n",
       "                      -1.3547e-02,  8.2344e-03, -6.8497e-03,  1.2642e-02, -4.3964e-02,\n",
       "                       4.4118e-02, -2.3159e-02,  2.3039e-02, -1.8662e-02, -1.3464e-02,\n",
       "                      -4.5041e-03,  8.3227e-03,  4.4026e-02, -3.9269e-02,  2.8998e-02,\n",
       "                      -2.7315e-02,  4.0734e-03, -2.0964e-02, -1.4755e-02,  3.5916e-02,\n",
       "                       9.1449e-03,  2.9204e-02,  1.8473e-02, -1.4103e-02, -9.2407e-05,\n",
       "                      -3.8738e-02,  3.3379e-03, -1.9426e-02,  2.8033e-02, -1.8245e-02,\n",
       "                       1.6622e-02,  1.2513e-02,  2.8437e-02,  2.4109e-02, -1.6011e-02,\n",
       "                      -3.5243e-02, -3.9884e-02,  3.5549e-02, -2.5840e-02, -1.6194e-02,\n",
       "                      -1.5910e-02,  1.9830e-02,  2.6102e-02, -4.2978e-02, -1.1099e-02,\n",
       "                       7.8341e-03,  4.1069e-02, -2.3185e-02, -3.5242e-02,  3.1194e-02,\n",
       "                       4.0943e-02,  2.7242e-02,  9.1605e-03, -2.8200e-03,  9.0792e-04,\n",
       "                       2.1323e-02,  1.1174e-02, -3.8210e-02, -2.8516e-02, -2.0737e-02,\n",
       "                       4.4507e-03, -3.7290e-02, -2.9173e-02,  3.0759e-02,  8.0670e-03,\n",
       "                       7.9363e-03,  1.5429e-02, -2.6317e-03,  3.0388e-02, -2.7049e-02,\n",
       "                       2.7818e-02,  2.3451e-03, -9.3674e-03, -1.3412e-02, -4.0295e-02,\n",
       "                      -2.3695e-02,  8.2279e-03,  8.6221e-03,  7.4489e-03,  2.0300e-02,\n",
       "                      -4.0861e-03,  1.1059e-02, -1.8271e-02,  3.5900e-02, -8.0023e-04,\n",
       "                       1.3399e-02, -2.2220e-02,  2.5496e-02,  2.5997e-04,  1.8094e-02,\n",
       "                      -1.1370e-03, -1.3829e-02,  2.1535e-02, -3.5989e-03,  8.1543e-03,\n",
       "                       1.2083e-02,  3.3152e-02, -3.9409e-02, -1.9520e-02, -1.3505e-02,\n",
       "                       2.4073e-02, -4.2656e-02,  3.3847e-02, -2.4737e-02, -1.5469e-02,\n",
       "                      -2.7262e-02, -4.2755e-02, -2.0564e-03,  1.2619e-02, -3.2922e-02,\n",
       "                      -1.3974e-02, -4.3668e-02, -1.1767e-02,  1.3667e-02, -1.3469e-02,\n",
       "                       3.3626e-02,  3.0448e-03, -6.0165e-03,  2.2638e-02,  1.2246e-02,\n",
       "                       2.1543e-02,  2.7804e-02,  3.6065e-02,  4.2699e-02,  2.2533e-03,\n",
       "                      -1.5141e-02,  1.0439e-03,  3.7591e-02, -6.5223e-03, -9.8913e-03,\n",
       "                      -3.8743e-02,  1.8771e-02,  2.4797e-02, -2.8011e-02,  3.9321e-02,\n",
       "                       4.3757e-02, -7.3042e-03, -2.8978e-02, -9.0868e-03,  3.4538e-02,\n",
       "                      -7.8181e-03,  1.5566e-02, -3.8035e-02,  1.1940e-02,  3.6683e-02,\n",
       "                       1.3519e-02, -4.1560e-02, -1.1626e-02,  4.0535e-02, -2.2680e-02,\n",
       "                      -1.0672e-02,  1.0407e-02, -3.1817e-02, -1.0547e-02, -2.4002e-02,\n",
       "                      -2.4530e-02,  3.5626e-02, -1.6315e-02, -1.7727e-02,  9.2414e-03,\n",
       "                      -1.7838e-02, -4.0018e-02,  4.3971e-02,  3.0151e-02, -3.8514e-02,\n",
       "                       1.2412e-02, -1.9943e-02, -3.1486e-03,  4.1381e-02,  2.6833e-02,\n",
       "                       3.7313e-03, -1.4141e-02, -1.9757e-03,  1.3653e-02,  3.2872e-02,\n",
       "                      -3.4319e-02,  1.3444e-02, -3.2228e-02, -3.0318e-02, -9.6931e-03,\n",
       "                       1.4994e-02, -2.7260e-02, -3.1066e-02, -3.9042e-02, -2.8635e-02,\n",
       "                      -2.8964e-02, -1.3050e-02,  4.0373e-02, -1.3064e-02,  3.1171e-02,\n",
       "                       2.6351e-02, -4.3323e-02,  4.2524e-02, -3.2794e-02,  3.1491e-02,\n",
       "                       4.3900e-02, -3.7766e-02, -3.4579e-02,  1.2626e-02, -5.5902e-03,\n",
       "                       2.7773e-03,  2.5427e-02], device='mps:0')),\n",
       "             ('layer_2_linear_relu.4.weight',\n",
       "              tensor([[ 0.0048,  0.0263, -0.0119,  ...,  0.0131, -0.0310, -0.0379],\n",
       "                      [-0.0130, -0.0257, -0.0295,  ...,  0.0207, -0.0405,  0.0297],\n",
       "                      [ 0.0440,  0.0017,  0.0109,  ..., -0.0043, -0.0194,  0.0048],\n",
       "                      ...,\n",
       "                      [ 0.0278,  0.0106, -0.0416,  ..., -0.0312,  0.0171,  0.0422],\n",
       "                      [-0.0007, -0.0079,  0.0084,  ..., -0.0302, -0.0242, -0.0041],\n",
       "                      [ 0.0391, -0.0124,  0.0339,  ...,  0.0142,  0.0224,  0.0417]],\n",
       "                     device='mps:0')),\n",
       "             ('layer_2_linear_relu.4.bias',\n",
       "              tensor([ 0.0267, -0.0221,  0.0250, -0.0238,  0.0324, -0.0386,  0.0414,  0.0123,\n",
       "                       0.0057, -0.0204], device='mps:0'))])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"my_ner_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = FashionMNISTClassifier().to(device)\n",
    "new_model.load_state_dict(torch.load(\"my_ner_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTClassifier(\n",
       "  (layer_1_flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (layer_2_linear_relu): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: T-shirt/top \n",
      " Actual: Ankle boot\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "new_model.eval()\n",
    "\n",
    "test_x_example = testing_data[0][0]\n",
    "test_y_example = testing_data[0][1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_x_example = test_x_example.to(device)\n",
    "\n",
    "    pred_logits = model(test_x_example)\n",
    "\n",
    "    prediction, actual = classes[pred_logits[0].argmax(axis=0)],  classes[test_y_example]\n",
    "    print(f\"Predicted: {prediction} \\n Actual: {actual}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builidng with Models and layer types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a Pytorch Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Pytorch model, it must inherit from the torch.nn.Module class to make sure it's compatible with Pytorch. The huge benefit is being able to track the weights (parameters) of each hidden layer. When assigned, the parameters become instances of the torch.nn.Parameter class which is a subclass of the torch.nn.Tensor class. As the weights (tensors) are updated the class will add them to a list of parameters based on the torch.nn.Module class. The weights can be accessed through the parameters() method from the torch.nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_layer = nn.Linear(in_features=5, out_features=10)\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.linear_layer_2 = nn.Linear(in_features=10, out_features=2)\n",
    "        self.softmax_layer = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        layer_1 = self.linear_layer(x)\n",
    "        layer_2 = self.activation_fn()(layer_1)\n",
    "        layer_3 = self.linear_layer_2(layer_2)\n",
    "        layer_4 = self.softmax_layer()(layer_3)\n",
    "        return layer_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearClassifierModel(\n",
       "  (linear_layer): Linear(in_features=5, out_features=10, bias=True)\n",
       "  (activation_fn): ReLU()\n",
       "  (linear_layer_2): Linear(in_features=10, out_features=2, bias=True)\n",
       "  (softmax_layer): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The displayed output inherits from nn.Module and also shows the model layers \n",
    "model = LinearClassifierModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0393,  0.2507, -0.4054, -0.0228,  0.4069],\n",
      "        [-0.4357,  0.4470, -0.0297,  0.2698, -0.2882],\n",
      "        [ 0.1768, -0.1114, -0.2653,  0.2500,  0.2786],\n",
      "        [ 0.2372,  0.2003, -0.1650, -0.3076,  0.2704],\n",
      "        [ 0.4116,  0.1751, -0.2421,  0.2375, -0.4243],\n",
      "        [-0.1958, -0.1220,  0.1047, -0.1787,  0.1606],\n",
      "        [ 0.1934,  0.0592,  0.1174, -0.1835,  0.0038],\n",
      "        [ 0.3793, -0.2489, -0.1857,  0.1972,  0.4322],\n",
      "        [ 0.4318,  0.3358,  0.1332, -0.2016, -0.3435],\n",
      "        [-0.2290, -0.3268, -0.4252,  0.3387,  0.3440]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2368, -0.3312,  0.1434, -0.0631,  0.4451,  0.2488, -0.1648,  0.0921,\n",
      "        -0.2638, -0.3270], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2418, -0.2944,  0.1352, -0.2306,  0.1630,  0.0178, -0.1322, -0.1990,\n",
      "          0.2017, -0.2476],\n",
      "        [ 0.1565,  0.2878, -0.1379,  0.1578,  0.2889,  0.2515, -0.2311,  0.2666,\n",
      "         -0.2227, -0.2847]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2682,  0.2633], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# The .parameters() method can be used since it's from the torch.nn.Module class\n",
    "for i in model.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=10, bias=True)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0393,  0.2507, -0.4054, -0.0228,  0.4069],\n",
      "        [-0.4357,  0.4470, -0.0297,  0.2698, -0.2882],\n",
      "        [ 0.1768, -0.1114, -0.2653,  0.2500,  0.2786],\n",
      "        [ 0.2372,  0.2003, -0.1650, -0.3076,  0.2704],\n",
      "        [ 0.4116,  0.1751, -0.2421,  0.2375, -0.4243],\n",
      "        [-0.1958, -0.1220,  0.1047, -0.1787,  0.1606],\n",
      "        [ 0.1934,  0.0592,  0.1174, -0.1835,  0.0038],\n",
      "        [ 0.3793, -0.2489, -0.1857,  0.1972,  0.4322],\n",
      "        [ 0.4318,  0.3358,  0.1332, -0.2016, -0.3435],\n",
      "        [-0.2290, -0.3268, -0.4252,  0.3387,  0.3440]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2368, -0.3312,  0.1434, -0.0631,  0.4451,  0.2488, -0.1648,  0.0921,\n",
      "        -0.2638, -0.3270], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in model.linear_layer.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every node is computed via y = wx+ b where w are the weights, x is the input, b is the bias, and y is the outpu value for that node in the neural network. Every node is connected to each other meaning the weights have an equal impact on those nodes. With an input of size m and an output of size n, the weights matrix would be size mxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=2, bias=True)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input 4x4 matrix -> 2x4 matrix\n",
    "linear_layer = Linear(in_features=4, out_features=2)\n",
    "linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "bias\n"
     ]
    }
   ],
   "source": [
    "for i in linear_layer.state_dict():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0393, -0.2453, -0.3070,  0.3471],\n",
      "        [ 0.0014,  0.0895, -0.3446,  0.2589]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3911, 0.4799], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 4x4 -> 2x4 Tensor (Weights) and 1x2 Tensor (Bias)\n",
    "# I.e. one bias per row of data\n",
    "for i in linear_layer.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6062, 0.1219, 0.5724, 0.0812],\n",
       "        [0.4876, 0.9861, 0.6739, 0.3195],\n",
       "        [0.8459, 0.2756, 0.7410, 0.1012],\n",
       "        [0.2101, 0.9687, 0.9727, 0.3913]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4x4 -> 4 samples (rows), 4 columns (features)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2375,  0.3155],\n",
       "        [ 0.0724,  0.4193],\n",
       "        [ 0.1644,  0.2766],\n",
       "        [-0.0011,  0.3330]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2x4 -> 4 samples (rows), \n",
    "# 2 columns (outputs: think of a binary classifier \n",
    "# where each of the 4 samples has a two \n",
    "# probablities for each class. \n",
    "# Therefore, it would need only a softmax \n",
    "# function to extract the highest logit \n",
    "# from each sample.\n",
    "y = linear_layer(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual matrix multiplication of x times the weights plus the bias will output the same y value above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = linear_layer.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]), torch.Size([2]), torch.Size([4, 4]))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape, bias.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.0393, -0.2453, -0.3070,  0.3471],\n",
       "         [ 0.0014,  0.0895, -0.3446,  0.2589]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.3911, 0.4799], requires_grad=True))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 50.],\n",
       "        [122.]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication with torch.linalg.multi_dot [(rows x columns)\n",
    "# This example shows matrix multiplication of a 2x3 and a 3x1 tensor to output a 2x1 matrix\n",
    "torch.linalg.multi_dot([torch.Tensor([[1,2,3], [4,5,6]]), torch.Tensor([[7],[8],[9]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1086, 0.2873],\n",
       "        [0.4056, 0.7241],\n",
       "        [0.3584, 0.5375],\n",
       "        [0.4207, 0.5751]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not correct\n",
    "# 2x4 = 2x4 x 4x4\n",
    "torch.linalg.multi_dot([weights, x]).T + bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2375,  0.3155],\n",
       "        [ 0.0724,  0.4193],\n",
       "        [ 0.1644,  0.2766],\n",
       "        [-0.0011,  0.3330]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct: Transpose the wieghts so it can be multipliced with input x\n",
    "# 4x2 = 4x4 x 4x2\n",
    "torch.linalg.multi_dot([x, weights.T]) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2375,  0.3155],\n",
       "        [ 0.0724,  0.4193],\n",
       "        [ 0.1644,  0.2766],\n",
       "        [-0.0011,  0.3330]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions layers are used to help extract high dimensional features like an image or text. Each layer has a kernal with a sliding window of size mxn or mxm. The sliding window uses a function to determine the output for a reduced window size based on the values in each sliding window. E.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "bias\n"
     ]
    }
   ],
   "source": [
    "# 2 pm cst "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Manipulation Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Max Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropout Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 2, 2, 3, 3, 3, 5, 5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = [1,1,2,2,2,3,3,3,4,5,5]\n",
    "nums.remove(nums[8])\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-chatbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
