{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification with Tensorflow Subclass Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GRU, Dense\n",
    "from sklearn.metrics import f1_score\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The Battle of Waterloo was fought in 1815.\",\n",
    "    \"Isaac Newton formulated the laws of motion.\",\n",
    "    \"The Renaissance period marked a significant cultural revival.\",\n",
    "    \"Einstein's theory of relativity revolutionized modern physics.\",\n",
    "    \"The French Revolution began in 1789.\",\n",
    "    \"Photosynthesis is the process by which plants convert light energy into chemical energy.\",\n",
    "    \"The Industrial Revolution transformed society with technological advancements.\",\n",
    "    \"Plate tectonics explain the movement of Earth's lithosphere.\",\n",
    "    \"Ancient Egyptians built the pyramids as tombs for their pharaohs.\",\n",
    "    \"DNA is the genetic material that carries hereditary information in living organisms.\",\n",
    "    \"World War II ended in 1945 with the defeat of Nazi Germany.\",\n",
    "    \"Charles Darwin proposed the theory of evolution by natural selection.\",\n",
    "    \"The invention of the printing press by Gutenberg revolutionized communication.\",\n",
    "    \"The Big Bang theory describes the origin of the universe.\",\n",
    "    \"The Great Wall of China was built over several centuries.\",\n",
    "    \"Atoms are the basic building blocks of matter.\",\n",
    "    \"The Black Death pandemic swept through Europe in the 14th century.\",\n",
    "    \"Albert Einstein's famous equation is E=mc^2.\",\n",
    "    \"The Declaration of Independence was signed in 1776.\",\n",
    "    \"The theory of continental drift was proposed by Alfred Wegener.\",\n",
    "    \"The ancient Greeks made significant contributions to mathematics and philosophy.\",\n",
    "    \"Newton's law of universal gravitation explains the attraction between objects.\",\n",
    "    \"The Roman Empire fell in 476 AD.\",\n",
    "    \"Chemical reactions involve the breaking and forming of chemical bonds.\",\n",
    "    \"The assassination of Archduke Franz Ferdinand triggered World War I.\",\n",
    "    \"Cells are the basic structural and functional units of all living organisms.\",\n",
    "    \"The Silk Road facilitated trade between Europe and Asia.\",\n",
    "    \"The discovery of penicillin by Alexander Fleming revolutionized medicine.\",\n",
    "    \"The American Civil War took place from 1861 to 1865.\",\n",
    "    \"The theory of special relativity explains the relationship between space and time.\",\n",
    "    \"The Magna Carta established the principle of limited government.\",\n",
    "    \"Gravity is the force that attracts objects toward each other.\",\n",
    "    \"The Maya civilization flourished in Mesoamerica.\",\n",
    "    \"The periodic table organizes elements based on their atomic number and properties.\",\n",
    "    \"The Cold War was a period of geopolitical tension between the United States and the Soviet Union.\",\n",
    "    \"Photosynthesis is vital for the production of oxygen on Earth.\",\n",
    "    \"The invention of the telescope revolutionized astronomy.\",\n",
    "    \"The Spanish Inquisition was established in the late 15th century.\",\n",
    "    \"Genetics is the study of heredity and variation in living organisms.\",\n",
    "    \"The Mongol Empire was the largest contiguous land empire in history.\",\n",
    "    \"Newton's laws of motion describe the behavior of objects in motion.\",\n",
    "    \"The Great Depression was a severe worldwide economic downturn in the 1930s.\",\n",
    "    \"The human brain is composed of billions of neurons.\",\n",
    "    \"The Trojan War is described in Homer's epic poem, the Iliad.\",\n",
    "    \"The discovery of fire revolutionized early human societies.\",\n",
    "    \"The theory of natural selection is a key mechanism of evolution.\",\n",
    "    \"The Treaty of Versailles ended World War I.\",\n",
    "    \"The theory of electromagnetism describes the relationship between electricity and magnetism.\",\n",
    "    \"The Mayflower Compact established self-government in Plymouth Colony.\",\n",
    "    \"The discovery of DNA's structure by Watson and Crick laid the foundation for molecular biology.\"\n",
    "]\n",
    "\n",
    "labels = [\"history\", \"science\", \"history\", \"science\", \"history\",\n",
    "          \"science\", \"history\", \"science\", \"history\", \"science\",\n",
    "          \"history\", \"science\", \"history\", \"science\", \"history\",\n",
    "          \"science\", \"history\", \"science\", \"history\", \"science\",\n",
    "          \"history\", \"science\", \"history\", \"science\", \"history\",\n",
    "          \"science\", \"history\", \"science\", \"history\", \"science\",\n",
    "          \"history\", \"science\", \"history\", \"science\", \"history\",\n",
    "          \"science\", \"history\", \"science\", \"history\", \"science\",\n",
    "          \"history\", \"science\", \"history\", \"science\", \"history\",\n",
    "          \"science\", \"history\", \"science\", \"history\", \"science\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'Sentences': sentences, 'Subject': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                           Sentences  Subject\n",
       " 0         The Battle of Waterloo was fought in 1815.  history\n",
       " 1        Isaac Newton formulated the laws of motion.  science\n",
       " 2  The Renaissance period marked a significant cu...  history\n",
       " 3  Einstein's theory of relativity revolutionized...  science\n",
       " 4               The French Revolution began in 1789.  history,\n",
       " (50, 2),\n",
       " Sentences    object\n",
       " Subject      object\n",
       " dtype: object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data_dict)\n",
    "data.head(), data.shape, data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Black Death pandemic swept through Europe ...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Maya civilization flourished in Mesoamerica.</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Big Bang theory describes the origin of th...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charles Darwin proposed the theory of evolutio...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The American Civil War took place from 1861 to...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences  Subject\n",
       "0  The Black Death pandemic swept through Europe ...  history\n",
       "1   The Maya civilization flourished in Mesoamerica.  history\n",
       "2  The Big Bang theory describes the origin of th...  science\n",
       "3  Charles Darwin proposed the theory of evolutio...  science\n",
       "4  The American Civil War took place from 1861 to...  history"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomize Data\n",
    "\n",
    "df = data.sample(frac=1, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into training and testing - 80, 20,\n",
    "\n",
    "# train_X = df.iloc[:40,0]\n",
    "# train_y = df.iloc[:40,1]\n",
    "# test_X = df.iloc[40:,0]\n",
    "# test_y = df.iloc[40:,1]\n",
    "# train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [the, black, death, pandemic, swept, through, ...\n",
       "1    [the, maya, civilization, flourished, in, meso...\n",
       "2    [the, big, bang, theory, describes, the, origi...\n",
       "3    [charles, darwin, proposed, the, theory, of, e...\n",
       "4    [the, american, civil, war, took, place, from,...\n",
       "Name: Sentences, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess data - tokenizing\n",
    "\n",
    "sentence_tokens = df['Sentences'].apply(lambda t: re.findall(r\"[\\w']+\", t.lower()))\n",
    "sentence_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the longest token in the sequence series\n",
    "max_token_length = max(sentence_tokens.apply(lambda x: len(x)))\n",
    "max_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine how many unique values there are in Categories column (we already know it is two, but this is to show just in case the count of unique values are unknown).\n",
    "\n",
    "unique_categories = len(df['Subject'].unique())\n",
    "unique_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the vocab first extract all the unique words into a single list\n",
    "unique_tokens = set()\n",
    "\n",
    "for sentence in sentence_tokens:\n",
    "    for word in sentence:\n",
    "\n",
    "        unique_tokens.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of vocab       \n",
    "vocab = len(unique_tokens)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'science'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Subject'].unique()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem': 0,\n",
       " 'ii': 1,\n",
       " 'involve': 2,\n",
       " 'behavior': 3,\n",
       " 'organisms': 4,\n",
       " 'electricity': 5,\n",
       " 'evolution': 6,\n",
       " 'universal': 7,\n",
       " 'earth': 8,\n",
       " 'inquisition': 9,\n",
       " 'forming': 10,\n",
       " 'self': 11,\n",
       " 'equation': 12,\n",
       " 'crick': 13,\n",
       " 'information': 14,\n",
       " 'mechanism': 15,\n",
       " 'bonds': 16,\n",
       " 'ended': 17,\n",
       " 'flourished': 18,\n",
       " 'dna': 19,\n",
       " 'land': 20,\n",
       " 'that': 21,\n",
       " 'contiguous': 22,\n",
       " 'plants': 23,\n",
       " 'production': 24,\n",
       " 'between': 25,\n",
       " 'built': 26,\n",
       " 'building': 27,\n",
       " 'neurons': 28,\n",
       " 'penicillin': 29,\n",
       " 'special': 30,\n",
       " 'cold': 31,\n",
       " 'plymouth': 32,\n",
       " 'trade': 33,\n",
       " 'cells': 34,\n",
       " 'relationship': 35,\n",
       " 'industrial': 36,\n",
       " 'structure': 37,\n",
       " '1930s': 38,\n",
       " 'is': 39,\n",
       " 'world': 40,\n",
       " 'are': 41,\n",
       " 'alfred': 42,\n",
       " 'telescope': 43,\n",
       " 'study': 44,\n",
       " 'transformed': 45,\n",
       " 'mathematics': 46,\n",
       " 'gravitation': 47,\n",
       " 'maya': 48,\n",
       " 'e': 49,\n",
       " 'ancient': 50,\n",
       " 'principle': 51,\n",
       " 'elements': 52,\n",
       " 'based': 53,\n",
       " 'tension': 54,\n",
       " 'triggered': 55,\n",
       " 'light': 56,\n",
       " 'atoms': 57,\n",
       " 'fire': 58,\n",
       " 'colony': 59,\n",
       " '476': 60,\n",
       " 'revival': 61,\n",
       " 'energy': 62,\n",
       " 'revolution': 63,\n",
       " 'severe': 64,\n",
       " 'trojan': 65,\n",
       " 'biology': 66,\n",
       " 'oxygen': 67,\n",
       " 'by': 68,\n",
       " 'magna': 69,\n",
       " 'epic': 70,\n",
       " 'fell': 71,\n",
       " 'economic': 72,\n",
       " '1789': 73,\n",
       " 'ad': 74,\n",
       " 'continental': 75,\n",
       " 'mc': 76,\n",
       " '14th': 77,\n",
       " '2': 78,\n",
       " 'assassination': 79,\n",
       " 'silk': 80,\n",
       " 'as': 81,\n",
       " 'hereditary': 82,\n",
       " 'universe': 83,\n",
       " 'late': 84,\n",
       " 'magnetism': 85,\n",
       " 'centuries': 86,\n",
       " 'marked': 87,\n",
       " 'franz': 88,\n",
       " 'asia': 89,\n",
       " 'heredity': 90,\n",
       " 'wall': 91,\n",
       " 'cultural': 92,\n",
       " 'empire': 93,\n",
       " 'space': 94,\n",
       " 'century': 95,\n",
       " 'history': 96,\n",
       " 'big': 97,\n",
       " 'time': 98,\n",
       " 'into': 99,\n",
       " 'basic': 100,\n",
       " 'printing': 101,\n",
       " '15th': 102,\n",
       " 'through': 103,\n",
       " 'was': 104,\n",
       " 'pandemic': 105,\n",
       " 'organizes': 106,\n",
       " 'geopolitical': 107,\n",
       " 'established': 108,\n",
       " 'union': 109,\n",
       " 'contributions': 110,\n",
       " 'archduke': 111,\n",
       " 'a': 112,\n",
       " \"earth's\": 113,\n",
       " 'mesoamerica': 114,\n",
       " 'carries': 115,\n",
       " 'black': 116,\n",
       " 'gravity': 117,\n",
       " 'made': 118,\n",
       " 'of': 119,\n",
       " \"homer's\": 120,\n",
       " 'signed': 121,\n",
       " 'largest': 122,\n",
       " 'several': 123,\n",
       " 'revolutionized': 124,\n",
       " '1815': 125,\n",
       " 'mayflower': 126,\n",
       " 'origin': 127,\n",
       " 'explain': 128,\n",
       " 'battle': 129,\n",
       " 'mongol': 130,\n",
       " 'key': 131,\n",
       " 'other': 132,\n",
       " 'carta': 133,\n",
       " 'composed': 134,\n",
       " 'molecular': 135,\n",
       " 'describe': 136,\n",
       " 'toward': 137,\n",
       " 'began': 138,\n",
       " 'early': 139,\n",
       " 'iliad': 140,\n",
       " 'over': 141,\n",
       " 'process': 142,\n",
       " 'each': 143,\n",
       " \"einstein's\": 144,\n",
       " 'society': 145,\n",
       " 'table': 146,\n",
       " 'renaissance': 147,\n",
       " 'greeks': 148,\n",
       " 'germany': 149,\n",
       " 'described': 150,\n",
       " 'technological': 151,\n",
       " 'chemical': 152,\n",
       " 'from': 153,\n",
       " 'fought': 154,\n",
       " 'motion': 155,\n",
       " 'tombs': 156,\n",
       " 'communication': 157,\n",
       " 'treaty': 158,\n",
       " 'waterloo': 159,\n",
       " 'formulated': 160,\n",
       " 'variation': 161,\n",
       " 'facilitated': 162,\n",
       " 'pyramids': 163,\n",
       " 'watson': 164,\n",
       " 'objects': 165,\n",
       " 'american': 166,\n",
       " 'period': 167,\n",
       " 'great': 168,\n",
       " 'lithosphere': 169,\n",
       " 'significant': 170,\n",
       " 'astronomy': 171,\n",
       " 'material': 172,\n",
       " 'for': 173,\n",
       " 'defeat': 174,\n",
       " 'societies': 175,\n",
       " 'billions': 176,\n",
       " 'medicine': 177,\n",
       " 'advancements': 178,\n",
       " 'took': 179,\n",
       " 'charles': 180,\n",
       " 'laid': 181,\n",
       " 'soviet': 182,\n",
       " 'government': 183,\n",
       " 'on': 184,\n",
       " 'natural': 185,\n",
       " 'their': 186,\n",
       " 'discovery': 187,\n",
       " 'movement': 188,\n",
       " 'units': 189,\n",
       " 'press': 190,\n",
       " 'compact': 191,\n",
       " 'united': 192,\n",
       " \"dna's\": 193,\n",
       " 'photosynthesis': 194,\n",
       " 'to': 195,\n",
       " 'newton': 196,\n",
       " 'living': 197,\n",
       " 'human': 198,\n",
       " 'nazi': 199,\n",
       " 'europe': 200,\n",
       " 'explains': 201,\n",
       " 'with': 202,\n",
       " 'pharaohs': 203,\n",
       " 'civil': 204,\n",
       " 'downturn': 205,\n",
       " 'swept': 206,\n",
       " '1865': 207,\n",
       " 'genetics': 208,\n",
       " 'which': 209,\n",
       " 'tectonics': 210,\n",
       " 'functional': 211,\n",
       " 'versailles': 212,\n",
       " '1776': 213,\n",
       " 'death': 214,\n",
       " 'atomic': 215,\n",
       " 'ferdinand': 216,\n",
       " 'declaration': 217,\n",
       " 'physics': 218,\n",
       " 'attracts': 219,\n",
       " 'relativity': 220,\n",
       " 'darwin': 221,\n",
       " 'theory': 222,\n",
       " 'attraction': 223,\n",
       " '1945': 224,\n",
       " 'spanish': 225,\n",
       " 'road': 226,\n",
       " 'blocks': 227,\n",
       " '1861': 228,\n",
       " 'foundation': 229,\n",
       " 'force': 230,\n",
       " 'drift': 231,\n",
       " 'reactions': 232,\n",
       " 'plate': 233,\n",
       " 'selection': 234,\n",
       " 'roman': 235,\n",
       " 'depression': 236,\n",
       " 'electromagnetism': 237,\n",
       " 'all': 238,\n",
       " 'albert': 239,\n",
       " 'proposed': 240,\n",
       " 'genetic': 241,\n",
       " 'modern': 242,\n",
       " 'laws': 243,\n",
       " 'describes': 244,\n",
       " \"newton's\": 245,\n",
       " 'in': 246,\n",
       " 'worldwide': 247,\n",
       " 'civilization': 248,\n",
       " 'wegener': 249,\n",
       " 'egyptians': 250,\n",
       " 'matter': 251,\n",
       " 'fleming': 252,\n",
       " 'periodic': 253,\n",
       " 'invention': 254,\n",
       " 'philosophy': 255,\n",
       " 'breaking': 256,\n",
       " 'china': 257,\n",
       " 'french': 258,\n",
       " 'structural': 259,\n",
       " 'bang': 260,\n",
       " 'gutenberg': 261,\n",
       " 'vital': 262,\n",
       " 'isaac': 263,\n",
       " 'place': 264,\n",
       " 'convert': 265,\n",
       " 'famous': 266,\n",
       " 'the': 267,\n",
       " 'limited': 268,\n",
       " 'states': 269,\n",
       " 'alexander': 270,\n",
       " 'properties': 271,\n",
       " 'independence': 272,\n",
       " 'law': 273,\n",
       " 'war': 274,\n",
       " 'i': 275,\n",
       " 'brain': 276,\n",
       " 'number': 277,\n",
       " 'and': 278}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, create the ids to tokens and tokens to ids (known as the vocab)\n",
    "\n",
    "# Also, be sure to increment the id by n amount to account for special tokens.\n",
    "# In this case, the only additional token is 0 which will indicate padding (<PAD>)\n",
    "\n",
    "id_to_token = {k:v.lower() for k,v in enumerate(unique_tokens)}\n",
    "# id_to_token[vocab+unique_categories] = \"<PAD>\"\n",
    "# id_to_token[unique_categories-2] = df['Subject'].unique()[0]\n",
    "# id_to_token[unique_categories-1] = df['Subject'].unique()[1]\n",
    "\n",
    "\n",
    "token_to_id = {v.lower():k for k,v in enumerate(unique_tokens)}\n",
    "# token_to_id[\"<PAD>\"] = vocab+unique_categories\n",
    "# token_to_id[df['Subject'].unique()[0]] = unique_categories-2\n",
    "# token_to_id[df['Subject'].unique()[1]] = unique_categories-1\n",
    "\n",
    "\n",
    "token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each token in every sequence to token ids\n",
    "\n",
    "all_sentence_ids = []\n",
    "\n",
    "for sentence in sentence_tokens:\n",
    "    per_sentence_ids = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        per_sentence_ids.append(token_to_id[word])\n",
    "        \n",
    "    if len(per_sentence_ids) < max_token_length:\n",
    "        \n",
    "        # Preprocess data - padding and/or truncation\n",
    "        per_sentence_ids.extend(np.zeros(max_token_length - len(sentence), dtype=float))\n",
    "\n",
    "    all_sentence_ids.append(per_sentence_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[267, 116, 214, 105, 206, 103, 200, 246, 267,  77,  95,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267,  48, 248,  18, 246, 114,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267,  97, 260, 222, 244, 267, 127, 119, 267,  83,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [180, 221, 240, 267, 222, 119,   6,  68, 185, 234,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 166, 204, 274, 179, 264, 153, 228, 195, 207,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 235,  93,  71, 246,  60,  74,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [208,  39, 267,  44, 119,  90, 278, 161, 246, 197,   4,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 254, 119, 267,  43, 124, 171,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [245, 273, 119,   7,  47, 201, 267, 223,  25, 165,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 187, 119,  58, 124, 139, 198, 175,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267,  79, 119, 111,  88, 216,  55,  40, 274, 275,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 130,  93, 104, 267, 122,  22,  20,  93, 246,  96,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [ 40, 274,   1,  17, 246, 224, 202, 267, 174, 119, 199, 149,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267,  36,  63,  45, 145, 202, 151, 178,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [117,  39, 267, 230,  21, 219, 165, 137, 143, 132,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [144, 222, 119, 220, 124, 242, 218,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 222, 119,  75, 231, 104, 240,  68,  42, 249,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267,  69, 133, 108, 267,  51, 119, 268, 183,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267,  31, 274, 104, 112, 167, 119, 107,  54,  25, 267, 192, 269,\n",
       "        278, 267, 182, 109],\n",
       "       [267, 158, 119, 212,  17,  40, 274, 275,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [239, 144, 266,  12,  39,  49,  76,  78,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 253, 146, 106,  52,  53, 184, 186, 215, 277, 278, 271,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267,  65, 274,  39, 150, 246, 120,  70,   0, 267, 140,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 168,  91, 119, 257, 104,  26, 141, 123,  86,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 198, 276,  39, 134, 119, 176, 119,  28,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 225,   9, 104, 108, 246, 267,  84, 102,  95,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267,  80, 226, 162,  33,  25, 200, 278,  89,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [ 57,  41, 267, 100,  27, 227, 119, 251,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [233, 210, 128, 267, 188, 119, 113, 169,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 187, 119,  29,  68, 270, 252, 124, 177,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [ 34,  41, 267, 100, 259, 278, 211, 189, 119, 238, 197,   4,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267,  50, 148, 118, 170, 110, 195,  46, 278, 255,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [ 50, 250,  26, 267, 163,  81, 156, 173, 186, 203,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 254, 119, 267, 101, 190,  68, 261, 124, 157,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 129, 119, 159, 104, 154, 246, 125,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 147, 167,  87, 112, 170,  92,  61,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 222, 119, 185, 234,  39, 112, 131,  15, 119,   6,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 222, 119,  30, 220, 201, 267,  35,  25,  94, 278,  98,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 168, 236, 104, 112,  64, 247,  72, 205, 246, 267,  38,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 126, 191, 108,  11, 183, 246,  32,  59,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [152, 232,   2, 267, 256, 278,  10, 119, 152,  16,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 187, 119, 193,  37,  68, 164, 278,  13, 181, 267, 229, 173,\n",
       "        135,  66,   0,   0],\n",
       "       [194,  39, 267, 142,  68, 209,  23, 265,  56,  62,  99, 152,  62,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 258,  63, 138, 246,  73,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 217, 119, 272, 104, 121, 246, 213,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [267, 222, 119, 237, 244, 267,  35,  25,   5, 278,  85,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [ 19,  39, 267, 241, 172,  21, 115,  82,  14, 246, 197,   4,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [245, 243, 119, 155, 136, 267,   3, 119, 165, 246, 155,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [194,  39, 262, 173, 267,  24, 119,  67, 184,   8,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [263, 196, 160, 267, 243, 119, 155,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the features create the input ids (where each index represents a token based on the vocab) into a tensor with the padding included\n",
    "\n",
    "input_ids = np.array(all_sentence_ids, dtype=int)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['history', 'science']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the unique classes\n",
    "unique_subject_names = df['Subject'].unique().tolist()\n",
    "unique_subject_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to ids for each label\n",
    "# label_ids = df['Subject'].apply(lambda e: 0 if e == 'history' else 1 )\n",
    "# label_ids\n",
    "\n",
    "all_label_ids = []\n",
    "for subject in df['Subject']:\n",
    "    all_label_ids.append(unique_subject_names.index(subject))\n",
    "\n",
    "# label_ids = torch.Tensor(all_label_ids)   \n",
    "label_ids = np.array(all_label_ids, dtype=int)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 17), (40,), (10, 17), (10,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training and testing\n",
    "\n",
    "train_X = input_ids[:40,:]\n",
    "train_y = label_ids[:40]\n",
    "test_X = input_ids[40:,:]\n",
    "test_y = label_ids[40:]\n",
    "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "class SubjectClassifier(tf.keras.Model):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int):\n",
    "        super().__init__(self)\n",
    "\n",
    "        self.embedding_layer = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_layer = LSTM(hidden_dim)\n",
    "        self.dense_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "\n",
    "    def __call__(self, inputs, training=None):\n",
    "        layer_1 = self.embedding_layer(inputs)\n",
    "        layer_2 = self.lstm_layer(layer_1)\n",
    "        layer_3 = self.dense_layer(layer_2)\n",
    "        return layer_3\n",
    "    \n",
    "\n",
    "model = SubjectClassifier(vocab, 100, 256)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 0.6988 - accuracy: 0.3750 - val_loss: 0.6954 - val_accuracy: 0.3750\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6923 - accuracy: 0.5000 - val_loss: 0.6822 - val_accuracy: 0.6250\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6758 - accuracy: 0.5625 - val_loss: 0.6560 - val_accuracy: 0.7500\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3247 - accuracy: 1.0000 - val_loss: 0.7166 - val_accuracy: 0.6250\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1141 - accuracy: 0.9688 - val_loss: 1.1539 - val_accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x299d7b070>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "model.fit(train_X, train_y, epochs=5, batch_size=4, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40    Chemical reactions involve the breaking and fo...\n",
       " 41    The discovery of DNA's structure by Watson and...\n",
       " 42    Photosynthesis is the process by which plants ...\n",
       " 43                 The French Revolution began in 1789.\n",
       " 44    The Declaration of Independence was signed in ...\n",
       " 45    The theory of electromagnetism describes the r...\n",
       " 46    DNA is the genetic material that carries hered...\n",
       " 47    Newton's laws of motion describe the behavior ...\n",
       " 48    Photosynthesis is vital for the production of ...\n",
       " 49          Isaac Newton formulated the laws of motion.\n",
       " Name: Sentences, dtype: object,\n",
       " 40    science\n",
       " 41    science\n",
       " 42    science\n",
       " 43    history\n",
       " 44    history\n",
       " 45    science\n",
       " 46    science\n",
       " 47    history\n",
       " 48    science\n",
       " 49    science\n",
       " Name: Subject, dtype: object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = df.iloc[40:,0]\n",
    "test_labels = df.iloc[40:,1]\n",
    "\n",
    "test_features, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 241ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.73724544e-01],\n",
       "       [6.08532727e-01],\n",
       "       [9.63613451e-01],\n",
       "       [1.11129855e-04],\n",
       "       [6.50132424e-04],\n",
       "       [9.84253705e-01],\n",
       "       [1.70536339e-04],\n",
       "       [9.83704388e-01],\n",
       "       [1.40580887e-04],\n",
       "       [9.82097447e-01]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "\n",
    "predictions = model.predict(test_X)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the discrete label based on the logits\n",
    "predictions_round = (predictions > .5).astype(int)\n",
    "predictions_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label:  science   Prediction:  science   Test Sequence:  Chemical reactions involve the breaking and forming of chemical bonds.\n",
      "Actual Label:  science   Prediction:  science   Test Sequence:  The discovery of DNA's structure by Watson and Crick laid the foundation for molecular biology.\n",
      "Actual Label:  science   Prediction:  science   Test Sequence:  Photosynthesis is the process by which plants convert light energy into chemical energy.\n",
      "Actual Label:  history   Prediction:  history   Test Sequence:  The French Revolution began in 1789.\n",
      "Actual Label:  history   Prediction:  history   Test Sequence:  The Declaration of Independence was signed in 1776.\n",
      "Actual Label:  science   Prediction:  science   Test Sequence:  The theory of electromagnetism describes the relationship between electricity and magnetism.\n",
      "Actual Label:  science   Prediction:  history   Test Sequence:  DNA is the genetic material that carries hereditary information in living organisms.\n",
      "Actual Label:  history   Prediction:  science   Test Sequence:  Newton's laws of motion describe the behavior of objects in motion.\n",
      "Actual Label:  science   Prediction:  history   Test Sequence:  Photosynthesis is vital for the production of oxygen on Earth.\n",
      "Actual Label:  science   Prediction:  science   Test Sequence:  Isaac Newton formulated the laws of motion.\n"
     ]
    }
   ],
   "source": [
    "for label, feature, pred in zip(test_labels, test_features, predictions_round):\n",
    "    print('Actual Label: ', label, '  Prediction: ', unique_subject_names[pred[0]], '  Test Sequence: ', feature)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7692307692307692\n"
     ]
    }
   ],
   "source": [
    "# Compute F1 Score\n",
    "print('F1 Score: ', f1_score(predictions_round, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-chatbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
